{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop RAW images\n",
    "```bash\n",
    "dcraw -q 3 -4 -T -o 1 data/door_stack/*.nef\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearize rendered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = natsorted(Path('data/door_stack/door_stack').glob('*.jpg'))\n",
    "num_images = len(image_paths)\n",
    "height, width = cv2.imread(str(image_paths[0]), cv2.IMREAD_UNCHANGED).shape[:2]\n",
    "channel = 3\n",
    "selected_index = torch.randperm(height * width)[:200].numpy().tolist()\n",
    "selected_pixels = [cv2.cvtColor(cv2.imread(str(p), cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB).reshape(-1, channel)[selected_index].flatten() for p in image_paths]\n",
    "selected_pixels = np.stack(selected_pixels, axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_times = np.arange(1, num_images + 1)\n",
    "exposure_times = 2 ** (exposure_times - 1)/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_min = 0.05\n",
    "z_max = 0.95\n",
    "def w_uniform(z, *_):\n",
    "    return torch.where(torch.logical_and(z >= z_min, z <= z_max), 1, 0)\n",
    "def w_tent(z, *_):\n",
    "    return torch.where(torch.logical_and(z >= z_min, z <= z_max), torch.min(z, 1 - z), 0)\n",
    "def w_gaussian(z, *_):\n",
    "    return torch.where(torch.logical_and(z >= z_min, z <= z_max), torch.exp(-4*(z - 0.5)**2/0.5**2), 0)\n",
    "def w_photon(z,t):\n",
    "    return torch.where(torch.logical_and(z >= z_min, z <= z_max), t, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "w_functions = {\n",
    "    'uniform': w_uniform,\n",
    "    'tent': w_tent,\n",
    "    'gaussian': w_gaussian,\n",
    "    'photon': w_photon\n",
    "}\n",
    "\n",
    "\n",
    "def gsolve(Z, B, l, weighting_function: Callable):\n",
    "    \"\"\"\n",
    "    Z: Tensor of shape (num_pixels, num_images)\n",
    "    B: Tensor of shape (num_images,)\n",
    "    l: Lambda, smoothness weight\n",
    "    weighting_function: Function to compute the weighting for each pixel value\n",
    "    Returns:\n",
    "    g: Tensor of shape (256,) - log exposure corresponding to pixel value z\n",
    "    lE: Tensor of shape (num_pixels,) - log irradiance at each pixel location\n",
    "    \"\"\"\n",
    "    Z = Z.long()  # Ensure pixel values are indices\n",
    "    n = 256\n",
    "    num_pixels, num_images = Z.shape\n",
    "    A_rows = num_pixels * num_images + n + 1\n",
    "    A_cols = n + num_pixels\n",
    "\n",
    "    A = torch.zeros((A_rows, A_cols), dtype=torch.float32)\n",
    "    b = torch.zeros((A_rows), dtype=torch.float32)\n",
    "\n",
    "    k = 0\n",
    "    for i in range(num_pixels):\n",
    "        for j in range(num_images):\n",
    "            z_ij = Z[i, j]\n",
    "            wij = weighting_function(z_ij.float()/255, torch.exp(B[j]))\n",
    "            A[k, z_ij] = wij\n",
    "            A[k, n + i] = -wij\n",
    "            b[k] = wij * B[j]\n",
    "            k += 1\n",
    "\n",
    "    # Fix the curve by setting its middle value to 0\n",
    "    A[k, 128] = 1\n",
    "    k += 1\n",
    "\n",
    "    # Smoothness equations\n",
    "    for i in range(0, n - 2):\n",
    "        w_i = weighting_function(torch.tensor(i / 255), None) if weighting_function != w_photon else 1\n",
    "        A[k, i] = l * w_i\n",
    "        A[k, i + 1] = -2 * l * w_i\n",
    "        A[k, i + 2] = l * w_i\n",
    "        k += 1\n",
    "\n",
    "    # Solve the system using SVD\n",
    "    b = b.view(-1, 1)\n",
    "    print(f\"Solving system of size A: {A.shape}, b: {b.shape}\")\n",
    "    x = torch.linalg.lstsq(A, b, driver = \"gelsd\").solution\n",
    "    print(x.shape)\n",
    "    x = x[:A_cols]\n",
    "\n",
    "    g = x[:n].squeeze()\n",
    "    lE = x[n:].squeeze()\n",
    "    \n",
    "    return g, lE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving system of size A: torch.Size([9857, 856]), b: torch.Size([9857, 1])\n",
      "torch.Size([856, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.7826e-08,\n",
       "         3.2187e-05,  9.1158e-06,  3.0875e-05,  4.8608e-05, -2.7180e-05,\n",
       "        -3.6851e-05,  2.6584e-05,  2.9802e-06, -2.9768e+00, -2.8815e+00,\n",
       "        -2.8117e+00, -2.7179e+00, -2.6056e+00, -2.5550e+00, -2.5446e+00,\n",
       "        -2.4420e+00, -2.3435e+00, -2.3265e+00, -2.2960e+00, -2.2154e+00,\n",
       "        -2.1233e+00, -2.1100e+00, -2.0936e+00, -2.0419e+00, -1.9957e+00,\n",
       "        -1.9719e+00, -1.9580e+00, -1.9166e+00, -1.9043e+00, -1.8516e+00,\n",
       "        -1.8055e+00, -1.7841e+00, -1.7667e+00, -1.7449e+00, -1.7311e+00,\n",
       "        -1.7008e+00, -1.6481e+00, -1.5968e+00, -1.5401e+00, -1.5495e+00,\n",
       "        -1.5001e+00, -1.4992e+00, -1.4797e+00, -1.4236e+00, -1.4200e+00,\n",
       "        -1.4026e+00, -1.3569e+00, -1.3647e+00, -1.3413e+00, -1.2937e+00,\n",
       "        -1.2825e+00, -1.2741e+00, -1.2778e+00, -1.2215e+00, -1.1990e+00,\n",
       "        -1.2587e+00, -1.2418e+00, -1.1585e+00, -1.1325e+00, -1.1279e+00,\n",
       "        -1.1128e+00, -1.0888e+00, -1.0727e+00, -1.0848e+00, -1.0650e+00,\n",
       "        -1.0013e+00, -1.0195e+00, -9.5921e-01, -9.2077e-01, -9.2885e-01,\n",
       "        -9.1310e-01, -8.5732e-01, -8.2769e-01, -7.9119e-01, -8.2728e-01,\n",
       "        -7.7199e-01, -7.7965e-01, -7.7120e-01, -7.1836e-01, -7.5083e-01,\n",
       "        -7.4958e-01, -6.7420e-01, -6.7797e-01, -6.5522e-01, -6.1725e-01,\n",
       "        -5.8685e-01, -6.2963e-01, -6.2086e-01, -6.1010e-01, -5.5873e-01,\n",
       "        -5.4447e-01, -5.5854e-01, -5.4322e-01, -5.0449e-01, -5.1041e-01,\n",
       "        -4.6193e-01, -4.8780e-01, -4.3302e-01, -4.4307e-01, -4.2983e-01,\n",
       "        -3.9116e-01, -4.0304e-01, -3.9890e-01, -3.4377e-01, -3.4424e-01,\n",
       "        -3.2154e-01, -3.1146e-01, -2.7162e-01, -2.7384e-01, -2.5032e-01,\n",
       "        -2.2404e-01, -1.6642e-01, -1.8436e-01, -1.3111e-01, -1.4682e-01,\n",
       "        -1.3681e-01, -8.3663e-02, -8.4883e-02, -7.3889e-02, -7.9468e-02,\n",
       "        -4.8583e-02, -4.9392e-02, -1.7686e-02,  2.4997e-06,  2.4349e-02,\n",
       "         1.4631e-03,  4.4431e-02,  7.4441e-02,  6.2109e-02,  9.3918e-02,\n",
       "         7.7474e-02,  8.1440e-02,  9.4009e-02,  1.0503e-01,  1.6858e-01,\n",
       "         1.5087e-01,  1.7476e-01,  2.2107e-01,  2.0888e-01,  2.2088e-01,\n",
       "         2.0935e-01,  2.7897e-01,  2.8061e-01,  3.2135e-01,  3.1522e-01,\n",
       "         2.8029e-01,  3.1869e-01,  3.7185e-01,  3.5758e-01,  3.6497e-01,\n",
       "         3.7659e-01,  4.3613e-01,  4.7666e-01,  4.8163e-01,  4.2707e-01,\n",
       "         5.3003e-01,  4.6671e-01,  5.7342e-01,  5.3508e-01,  5.7605e-01,\n",
       "         6.2973e-01,  5.7439e-01,  6.1364e-01,  5.8603e-01,  6.3408e-01,\n",
       "         6.6795e-01,  6.6820e-01,  6.4204e-01,  6.8819e-01,  7.5560e-01,\n",
       "         7.5459e-01,  7.4959e-01,  7.3671e-01,  7.2564e-01,  8.1601e-01,\n",
       "         7.7223e-01,  8.2854e-01,  8.0688e-01,  8.1871e-01,  9.0263e-01,\n",
       "         8.4403e-01,  8.8719e-01,  9.0935e-01,  8.8675e-01,  9.1772e-01,\n",
       "         9.2499e-01,  9.6548e-01,  8.9635e-01,  9.8207e-01,  1.0016e+00,\n",
       "         1.0483e+00,  1.0878e+00,  1.0430e+00,  1.0511e+00,  1.0549e+00,\n",
       "         1.1822e+00,  1.1943e+00,  1.2253e+00,  1.2276e+00,  1.1477e+00,\n",
       "         1.1859e+00,  1.1717e+00,  1.3260e+00,  1.2449e+00,  1.3004e+00,\n",
       "         1.2532e+00,  1.3567e+00,  1.3895e+00,  1.3877e+00,  1.3626e+00,\n",
       "         1.4191e+00,  1.4100e+00,  1.4032e+00,  1.4388e+00,  1.4390e+00,\n",
       "         1.4345e+00,  1.4800e+00,  1.5122e+00,  1.5370e+00,  1.5249e+00,\n",
       "         1.5023e+00,  1.5446e+00,  1.5111e+00,  1.6222e+00,  1.6201e+00,\n",
       "         1.6745e+00,  1.6974e+00,  1.6743e+00,  1.7346e+00,  1.7444e+00,\n",
       "         1.7790e+00,  1.8215e+00,  1.8512e+00,  1.8925e+00,  1.9220e+00,\n",
       "         1.9718e+00,  2.0154e+00,  1.9792e+00,  7.1526e-07, -1.2517e-06,\n",
       "         4.7684e-07,  1.9073e-06,  5.4836e-06, -9.5367e-06, -5.4836e-06,\n",
       "         9.0599e-06,  2.3842e-06,  1.0729e-06,  3.0994e-06,  7.6294e-06,\n",
       "         1.0967e-05])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'gaussian'\n",
    "lamb = 0\n",
    "\n",
    "g, lE = gsolve(torch.tensor(selected_pixels), torch.log(torch.tensor(exposure_times).float()), lamb, w_functions[method])\n",
    "g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
